coef(fit1)
coef(fit3)
coef(fit2)
summary(fit2)$coefficients
fit2 <- lm(mpg ~  wt + cyl + am, data = mtcars)
summary(fit2)
fit3 <- lm(mpg ~ wt + am, data = mtcars)
summary(fit3)
fit2 <- lm(mpg ~ wt + cyl + am, data = mtcars)
fit3 <- lm(mpg ~ wt + am, data = mtcars)
anova(fit1, fit2, fit3)
anova(fit1, fit3)
anova(fit1, fit2)
anova(fit1, fit2, fit3)
fit2 <- lm(mpg ~  wt + cyl + am + wt:am, data = mtcars)
summary(fit2)
fit3 <- lm(mpg ~ wt + am + wt:am, data = mtcars)
summary(fit3)
par(mfrow = c(2, 2), mar=c(1,1,1,1))
plot(fit2)
fit1 <- lm(mpg ~ am, data = mtcars)
summary(fit1)
t.test(mpg ~ am, data = mtcars)
plot(fit2, main= 'Fig 3.)
''
)
''
plot(fit2, main= 'Fig 3.')
plot(fit2, title= 'Fig 3.')
library(MASS)
head(shuttle)
dim(shuttle)
head(shuttle)
shuttle$newUse <- as.numeric(shuttle$use == "auto")
head(shuttle)
fit <- glm(newUse ~ as.factor(wind) - 1, data=shuttle, family="binomial")
odds <- exp(summary(fit)$coef)
odds[1] / odds[2] # 0.9686888
head(shuttle)
shuttle$newUse <- as.numeric(shuttle$use == "auto")
fit <- glm(newUse ~ as.factor(wind) - 1, data=shuttle, family="binomial")
exp(summary(fit)$coef)
fit <- glm(use ~ wind, family='binomial', shuttle)
exp(fit$coeff)
exp(summary(fit)$coef)
fit <- glm(use ~ wind, family='binomial', shuttle)
exp(fit$coeff)
shuttle$newUse <- as.numeric(shuttle$use == "auto")
fit <- glm(newUse ~ wind, family='binomial', shuttle)
exp(fit$coeff)
fit <- glm(use ~ wind, family='binomial', shuttle)
exp(fit$coeff)
fit <- glm(use ~ wind + as.factor(magn), family='binomial', shuttle)
exp(fit$coeff)
fit <- glm(use ~ wind + as.factor(magn), family='binomial', data=shuttle)
exp(fit$coeff)
fit <- glm(use ~ wind, family='binomial', data=shuttle)
exp(fit$coeff)
summary(fit)
shuttle$newUse <- as.numeric(shuttle$use == "auto")
fit <- glm(newUse ~ as.factor(wind) - 1, data=shuttle, family="binomial")
odds <- exp(summary(fit)$coef)
odds[1] / odds[2]
fit <- glm(newUse ~ wind - 1, data=shuttle, family="binomial")
odds <- exp(summary(fit)$coef)
odds[1] / odds[2] # 0.9686888
fit <- glm(newUse ~ wind + magn - 1, family="binomial", data=shuttle)
summary(fit)$coef
exp(coef(fit))
odds <- exp(cbind(OddsRatio = coef(fit), confint(fit)))
odds[1] / odds[2]
fit <- glm(use ~ wind + magn, family='binomial', data=shuttle)
exp(fit$coeff)
1.4384/1.4852
fit<-glm(newUse ~ wind + magn) - 1, family = binomial, data = shuttle2)
exp(coef(fit))
exp(cbind(OddsRatio = coef(fit), confint(fit)))
1.4384/1.4852
fit<-glm(newUse ~ wind, family = binomial, data = shuttle)
summary(fit)$coef
fit2<-glm(1 - newUse ~ wind, family = binomial, data = shuttle)
summary(fit2)$coef
dim(InsectSprays)
head(InsectSprays)
fitb <- glm(count ~ spray - 1, data=InsectSprays, family="poisson")
summary(fitb)$coef
rate <- exp(coef(fitb))
str(InsectSprays)
rate <- exp(coef(fitb))
rate
coef(fitb)
coef(fitb)
str(InsectSprays)
fitb2<-glm(count~spray-1,data=InsectSprays,family=poisson)
summary(fitb2)$coef
exp(coef(fitb2))
14.500  / 15.333
fitb<-glm(count ~ spray, family = poisson, data=InsectSprays, offset = log(count + 1))
summary(fitb)$coef
fitb2<-glm(count ~ factor(spray), family = poisson,data=InsectSprays,offset = log(10)+log(count+1))
summary(fitb2)$coef
fit <- glm(count ~ spray + offset(log(count+1)), family="poisson", data=InsectSprays)
fit2 <- glm(count ~ spray + offset(log(10)+log(count+1)),family="poisson", data=InsectSprays)
summary(fit)$coef
summary(fit2)$coef
x <- -5 : 5
y <- c(5.12, 3.93, 2.67, 1.87, 0.52, 0.08, 0.93, 2.05, 2.54, 3.87, 4.97)
knotPoint <- c(0)
knotPoint <- c(0)
spline <- sapply(knotPoint, function(knot) (x > knot) * (x - knot))
xMatrix <- cbind(1, x, spline)
fit <- lm(y ~ xMatrix - 1)
yhat <- predict(fit)
yhat
slope <- fit$coef[2] + fit$coef[3]
slope # 1.013
plot(x, y)
lines(x, yhat, col=2)
Problem 5.
# Suppose that we have created a machine learning algorithm that predicts whether a link will be
# clicked with 99% sensitivity and 99% specificity. The rate the link is clicked is 1/1000 of
# visits to a website. If we predict the link will be clicked on a specific visit,
# what is the probability it will actually be clicked?
# 100,000 visits => 100 clicks
# 99% = sensitivity = TP/(TP+FN) = 99/(99+1) = 99/100
# 99% specificity =TN/(TN+FP) = 98901/(98901+999) = 98901/99900
# P(actually clicked|clicked) = TP/(TP+FP) = 99/(99+999) = 9%
library(AppliedPredictiveModeling)
library(caret)
data(AlzheimerDisease)
library(AppliedPredictiveModeling)
library(caret)
library(AppliedPredictiveModeling)
library(AppliedPredictiveModeling)
library(caret)
install.packages("caret")
install.packages("AppliedPredictiveModeling")
library(AppliedPredictiveModeling)
library(caret)
library(ggplot2)
library(caret)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
trainIndex = createDataPartition(diagnosis, p = 0.50,list=FALSE)
training = adData[trainIndex,]
testing = adData[-trainIndex,]
training
?createDataPartition
data(concrete)
library(caret)
set.seed(1000)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[ inTrain,]
testing = mixtures[-inTrain,]
dim(training)
str(training)
hist(training$Superplasticizer,main="",xlab="Superplasticizer")
qplot(CompressiveStrength, Cement, data=concrete)
index<-colnames(concrete[,c(1,2,3,4,5,6,7)])
featurePlot(x=training[,index], y=training$CompressiveStrength, plot="pairs")
summary(training)
summary(training)
index
featurePlot(x=training[,1-8], y=training$CompressiveStrength, plot="pairs")
featurePlot(x=training[,1-8], y=training$CompressiveStrength, plot="pairs")
qplot(CompressiveStrength, Cement, data=concrete)
hist(training$Superplasticizer,main="",xlab="Superplasticizer")
summary(training)
library(AppliedPredictiveModeling)
data(concrete)
library(caret)
set.seed(1000)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[ inTrain,]
testing = mixtures[-inTrain,]
featurePlot(x=training[,1-8], y=training$CompressiveStrength, plot="pairs")
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
summary(training)
str(training)
ols_IL <- training[,grep('^IL', x = names(training) )]
Cols_IL <- training[,grep('^IL', x = names(training) )]
preObj <- preProcess(training[,Cols_IL],method=c("center","scale"))
preObj <- preProcess(Cols_IL, method=c("center","scale"))
preObj
ggplot(data = training, aes(x = Superplasticizer)) + geom_histogram() + theme_bw()
ggplot(data = training, aes(x = training$Superplasticizer)) + geom_histogram() + theme_bw()
library(ggplot2)
ggplot(data = training, aes(x = training$Superplasticizer))
qp <- ggplot(data = training, aes(x = training$Superplasticizer))
qp
qplot(data = training, aes(x = training$Superplasticizer)) + geom_histogram() + theme_bw()
ggplot(data = training, aes(x = training$Superplasticizer))
Cols_IL <- grep("^IL", names(training), value = TRUE)
Cols_IL <- grep("^IL", names(training), value = TRUE)
preObj <- preProcess(training[, Cols_IL], method = "pca", thresh = 0.9)
preObj
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
str(training)
Cols_IL <- grep("^IL", names(training), value = TRUE)
predictors_IL <- predictors[, IL_str]
adData
View(adData)
View(adData)
predictors_IL <- adData[, IL_str]
predictors_IL <- adData[, Cols_IL]
df2 <- data.frame(diagnosis, predictors_IL)
predictors_IL <- training[, Cols_IL]
df2 <- data.frame(diagnosis, predictors_IL)
print(cartModel$finalModel)
library(AppliedPredictiveModeling)
data(segmentationOriginal)
library(caret)
library(rpart)
df1 <- segmentationOriginal
trainIndex <- df1$Case == "Train"
training = df1[trainIndex, ]
testing = df1[!trainIndex, ]
set.seed(125)
##
cartModel <- train(Class ~ ., data=training, method="rpart")
cartModel$finalModel
print(cartModel$finalModel)
plot(cartModel$finalModel, uniform=TRUE)
text(cartModel$finalModel, use.n=TRUE, all=TRUE, cex=.8)
library(pgmm)
install.packages("pgmm")
library(pgmm)
data(olive)
olive = olive[,-1]
newdata = as.data.frame(t(colMeans(olive)))
fit <- train(Area~.,data=olive,method="rpart")
data(olive)
View(olive)
pred <- predict(fit,newdata)
pred
print(pred)
library(ElemStatLearn)
install.packages("ElemStatLearn")
library(ElemStatLearn)
data(SAheart)
set.seed(8484)
train = sample(1:dim(SAheart)[1],size=dim(SAheart)[1]/2,replace=F)
trainSA = SAheart[train,]
testSA = SAheart[-train,]
set.seed(13234)
modelSA <- train(chd ~ age + alcohol + obesity + tobacco + typea + ldl, data=trainSA, method="glm", family="binomial")
summary(modelSA)
missClass = function(values,prediction){sum(((prediction > 0.5)*1) != values)/length(values)}
missClass
predict.modelSA.test =  predict(modelSA, newdata=testSA)
table(predict.modelSA.test>0.5, testSA$chd)
missClass(testSA$chd, predict(model, newdata = testSA))
missClass(testSA$chd, predict(modelSA , newdata = testSA))
missClass(trainSA$chd, predict(modelSA , newdata = trainSA))
missClass(testSA$chd, predict(modelSA , newdata = testSA))
missClass(trainSA$chd, predict(modelSA , newdata = trainSA))
library(ElemStatLearn)
data(vowel.train)
data(vowel.test)
head(vowel.train)
head(vowel.test)
dim(vowel.train)
dim(vowel.test)
vowel.train$y <- as.factor(vowel.train$y)
vowel.test$y <- as.factor(vowel.test$y)
set.seed(33833)
modelRf <- randomForest(y ~ ., data = vowel.train, importance = FALSE)
install.packages("randomForest", dependencies = TRUE)
library(randomForest)
set.seed(33833)
modelRf <- randomForest(y ~ ., data = vowel.train, importance = FALSE)
order(varImp(modelRf), decreasing=T)
library(ElemStatLearn)
sessionInfo()
data(vowel.train)
data(vowel.test)
data(vowel.train)
vowel.train$y <- as.factor(vowel.train$y)
vowel.test$y <- as.factor(vowel.test$y)
set.seed(33833)
fitRf <- train(y ~ ., data=vowel.train, method="rf")
library(caret)
sessionInfo()
set.seed(33833)
fitRf <- train(y ~ ., data=vowel.train, method="rf")
library(randomForest)
fitRf <- train(y ~ ., data=vowel.train, method="rf")
fitRf
print(fitRF)
print(fitRf)
fitGBM <- train(y ~ ., data=vowel.train, method="gbm")
predRf <- predict(fitRf, vowel.test)
predGBM <- predict(fitGBM, vowel.test)
predRf
confusionMatrix(predRf, vowel.test$y)
confusionMatrix(predRf, vowel.test$y)$overall[1]
confusionMatrix(predRf, vowel.test$y)$overall['Accuracy']
confusionMatrix(predGBM vowel.test$y)$overall['Accuracy']
confusionMatrix(predGBM, vowel.test$y)$overall['Accuracy']
confusionMatrix(predRf, vowel.test$y)$overall['Accuracy']
confusionMatrix(predGBM, vowel.test$y)$overall['Accuracy']
data(vowel.train)
data(vowel.test)
data(vowel.train)
data(vowel.test)
vowel.train$y <- as.factor(vowel.train$y)
vowel.test$y <- as.factor(vowel.test$y)
set.seed(33833)
fitRf <- train(y ~ ., data=vowel.train, method="rf")
fitGBM <- train(y ~ ., data=vowel.train, method="gbm")
predRf <- predict(fitRf, vowel.test)
predGBM <- predict(fitGBM, vowel.test)
confusionMatrix(predRf, vowel.test$y)$overall['Accuracy']
confusionMatrix(predGBM, vowel.test$y)$overall['Accuracy']
predboth <- data.frame(predRf, predGBM, y=vowel.test$y, agree=predRf == predGBM)
accuracy <- sum(predRf[pred$agree] == pred$y[pred$agree]) / sum(pred$agree)
predboth
accuracy <- sum(predRf[predboth$agree] == predboth$y[predboth$agree]) / sum(predboth$agree)
accuracy
predboth <- (predRf == predGBM)
confusionMatrix(vowel.test$y[predboth], predRf[predboth])$overall['Accuracy']
library(caret)
library(gbm)
library(caret)
library(gbm)
set.seed(3433)
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
set.seed(62433)
fitRf <- train(diagnosis ~ ., data=training, method="rf")
fitGBM <- train(diagnosis ~ ., data=training, method="gbm")
fitLDA <- train(diagnosis ~ ., data=training, method="lda")
library(MASS)
fitLDA <- train(diagnosis ~ ., data=training, method="lda")
predRf <- predict(fitRf, testing)
predGBM <- predict(fitGBM, testing)
predLDA <- predict(fitLDA, testing)
predStack <- data.frame(predRf, predGBM, predLDA, diagnosis=testing$diagnosis)
fitAll <- train(diagnosis ~., data=predStack, method="rf")
predFit <- predict(fitAll, testing)
c1 <- confusionMatrix(predRf, testing$diagnosis)$overall[1]
c2 <- confusionMatrix(predGBM, testing$diagnosis)$overall[1]
c3 <- confusionMatrix(predLDA, testing$diagnosis)$overall[1]
c4 <- confusionMatrix(predFit, testing$diagnosis)$overall[1]
print(paste(c1, c2, c3, c4))
library(caret)
library(gbm)
set.seed(3433)
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
set.seed(62433)
fitRf <- train(diagnosis ~ ., data=training, method="rf")
fitGBM <- train(diagnosis ~ ., data=training, method="gbm")
library(MASS)
fitLDA <- train(diagnosis ~ ., data=training, method="lda")
predRf <- predict(fitRf, testing)
predGBM <- predict(fitGBM, testing)
predLDA <- predict(fitLDA, testing)
predStack <- data.frame(predRf, predGBM, predLDA, diagnosis=testing$diagnosis)
fitAll <- train(diagnosis ~., data=predStack, method="rf")
predFit <- predict(fitAll, testing)
c1 <- confusionMatrix(predRf, testing$diagnosis)$overall[1]
c2 <- confusionMatrix(predGBM, testing$diagnosis)$overall[1]
c3 <- confusionMatrix(predLDA, testing$diagnosis)$overall[1]
c4 <- confusionMatrix(predFit, testing$diagnosis)$overall[1]
print(paste(c1, c2, c3, c4))
set.seed(3523)
library(AppliedPredictiveModeling)
data(concrete)
inTrain = createDataPartition(concrete$CompressiveStrength, p = 3/4)[[1]]
training = concrete[ inTrain,]
testing = concrete[-inTrain,]
set.seed(233)
fit <- train(CompressiveStrength ~ ., data=training, method="lasso")
fit <- train(CompressiveStrength ~ ., data=training, method="lasso")
fit
plot.enet(fit$finalModel, xvar="penalty", use.color=T)
library(lubridate)
install.packages("lubridate")
library(lubridate)
dat = read.csv("https://d396qusza40orc.cloudfront.net/predmachlearn/gaData.csv")
fileUrl <- "https://d396qusza40orc.cloudfront.net/predmachlearn/gaData.csv"
download.file(fileUrl, destfile="./data/gaData.csv")
fileUrl <- "https://d396qusza40orc.cloudfront.net/predmachlearn/gaData.csv"
download.file(fileUrl, destfile="./data/gaData.csv")
library(lubridate)
dat = read.csv("./data/gaData.csv")
setwd("/Users/moiralennox/Documents/IT Documents/Data Science Class/DS8 - Machine Learning")
dat = read.csv("./Data/gaData.csv")
training = dat[year(dat$date) < 2012,]
testing = dat[(year(dat$date)) > 2011,]
tstrain = ts(training$visitsTumblr)
tstrain = ts(training$visitsTumblr)
head(tstrain)
dim(tstrain)
fit <- bats(tstrain)
library(forecast)
install.packages("forecast")
library(forecast)
fit <- bats(tstrain)
fit
pred <- forecast(fit, level=95, h=dim(testing)[1])
names(data.frame(pred))
predComb <- cbind(testing, data.frame(pred))
names(testing)
names(predComb)
predComb$in95 <- (predComb$Lo.95 < predComb$visitsTumblr) &
(predComb$visitsTumblr < predComb$Hi.95)
prop.table(table(predComb$in95))[2]
h <- dim(testing)[1]
fcast <- forecast(fit, level = 95, h = h)
accuracy(fcast, testing$visitsTumblr)
result <- c()
l <- length(fcast$lower)
for (i in 1:l){
x <- testing$visitsTumblr[i]
a <- fcast$lower[i] < x & x < fcast$upper[i]
result <- c(result, a)
}
result <- c()
l <- length(fcast$lower)
for (i in 1:l){
x <- testing$visitsTumblr[i]
a <- fcast$lower[i] < x & x < fcast$upper[i]
result <- c(result, a)
}
sum(result)/l * 100
result <- c()
l <- length(fcast$lower)
for (i in 1:l){
x <- testing$visitsTumblr[i]
a <- fcast$lower[i] < x & x < fcast$upper[i]
result <- c(result, a)
}
for (i in 1:l){
x <- testing$visitsTumblr[i]
a <- fcast$lower[i] < x & x < fcast$upper[i]
result <- c(result, a)
}
for (i in 1:l){
x <- testing$visitsTumblr[i]
a <- fcast$lower[i] < x & x < fcast$upper[i]
result <- c(result, a)
}
pred <- forecast(fit, length(testing$visitsTumblr))
pred <- cbind(testing, data.frame(pred))
pred$isIn95 <- pred$Lo.95 < pred$visitsTumblr & pred$visitsTumblr < pred$Hi.95
prop.table(table(pred$isIn95))
set.seed(325)
set.seed(3523)
library(AppliedPredictiveModeling)
data(concrete)
inTrain = createDataPartition(concrete$CompressiveStrength, p = 3/4)[[1]]
training = concrete[ inTrain,]
testing = concrete[-inTrain,]
set.seed(325)
require(e1071)
fit <- svm(CompressiveStrength ~ ., data=training)
testing$hat <- predict(fit, testing)
testing$error <- testing$CompressiveStrength - testing$hat
rmse <- sqrt(mean(testing$error ^ 2))
rmse
set.seed(325)
fit <- svm(CompressiveStrength ~., data=training)
pred <- predict(fit, testing)
acc <- accuracy(pred, testing$CompressiveStrength)
acc
install.packages("devtools")
library(devtools)
install_github('slidify', 'ramnathv')
install_github('slidifyLibraries', 'ramnathv')
library(slidify)
library(slidify)
runApp(display.mode='showcase')
shiny::runApp('~/Documents/IT Documents/Data Science Class/DS9 - Developing Data Products/CarsApp')
runApp(display.mode='showcase')
setwd("~/Documents/IT Documents/Data Science Class/DS9 - Developing Data Products/shiny_app1")
runApp(display.mode='showcase')
setwd("~/GitHub/DevelopingDataProducts/ProjectApp")
runApp(display.mode='showcase')
setwd("~/Documents/IT Documents/Data Science Class/DS9 - Developing Data Products/CarsApp")
runApp(display.mode='showcase')
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
